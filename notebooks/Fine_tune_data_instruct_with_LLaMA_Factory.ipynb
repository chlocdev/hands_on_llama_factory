{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use LLaMA-Factory to fine-tune already available instruct data use LoRA technique"
      ],
      "metadata": {
        "id": "v5mbUSqjeg2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gNi10Quj8MT",
        "outputId": "e8d7e74b-0b57-4e54-f67c-52f4ff56601b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug  3 09:26:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0              42W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVQzzjcabe2-",
        "outputId": "198f28bd-77cc-40b2-ea0b-b035b42e65db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15962, done.\u001b[K\n",
            "remote: Counting objects: 100% (6780/6780), done.\u001b[K\n",
            "remote: Compressing objects: 100% (276/276), done.\u001b[K\n",
            "remote: Total 15962 (delta 6517), reused 6506 (delta 6504), pack-reused 9182\u001b[K\n",
            "Receiving objects: 100% (15962/15962), 222.36 MiB | 20.29 MiB/s, done.\n",
            "Resolving deltas: 100% (11889/11889), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.6/318.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m101.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Dependencies\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "%rm -rf LLaMA-Factory\n",
        "\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "\n",
        "%cd LLaMA-Factory\n",
        "\n",
        "%ls\n",
        "\n",
        "!pip install -q -e \".[torch,metrics]\"\n",
        "\n",
        "#!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show help information.\n",
        "!llamafactory-cli help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QkJVT4KdDbc",
        "outputId": "5c2e52aa-d5f6-420f-faff-1067dbf4bb3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-03 09:28:09.279752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-08-03 09:28:09.297192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-03 09:28:09.318152: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-03 09:28:09.324589: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-03 09:28:09.339783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-03 09:28:10.612732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "----------------------------------------------------------------------\n",
            "| Usage:                                                             |\n",
            "|   llamafactory-cli api -h: launch an OpenAI-style API server       |\n",
            "|   llamafactory-cli chat -h: launch a chat interface in CLI         |\n",
            "|   llamafactory-cli eval -h: evaluate models                        |\n",
            "|   llamafactory-cli export -h: merge LoRA adapters and export model |\n",
            "|   llamafactory-cli train -h: train models                          |\n",
            "|   llamafactory-cli webchat -h: launch a chat interface in Web UI   |\n",
            "|   llamafactory-cli webui: launch LlamaBoard                        |\n",
            "|   llamafactory-cli version: show version info                      |\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/LLaMA-Factory/src')\n",
        "\n",
        "import llamafactory\n",
        "\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "print(\"import Successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QQCZjUhdX66",
        "outputId": "ef2f299c-7ca3-4c5d-fb9a-3ebb9b5fe31d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import Successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check pretrain-data example dataset\n",
        "example_dataset_path = \"/content/LLaMA-Factory/data/glaive_toolcall_en_demo.json\" # \"/content/LLaMA-Factory/data/alpaca_en_demo.json\"\n",
        "\n",
        "with open(example_dataset_path,\"r\") as f:\n",
        "  example_dataset = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(example_dataset)}\")\n",
        "\n",
        "print(example_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IENza8aydkb4",
        "outputId": "5c6ccc2a-83ce-4885-93f0-0596e0a77245"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 300\n",
            "{'conversations': [{'from': 'human', 'value': 'Hi, I have some ingredients and I want to cook something. Can you help me find a recipe?'}, {'from': 'gpt', 'value': 'Of course! I can help you with that. Please tell me what ingredients you have.'}, {'from': 'human', 'value': 'I have chicken, bell peppers, and rice.'}, {'from': 'function_call', 'value': '{\"name\": \"search_recipes\", \"arguments\": {\"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"]}}'}, {'from': 'observation', 'value': '{\"recipes\": [{\"name\": \"Chicken and Bell Pepper Stir Fry\", \"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"], \"instructions\": \"Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice.\"}, {\"name\": \"Chicken and Rice Casserole\", \"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"], \"instructions\": \"Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown.\"}]}'}, {'from': 'gpt', 'value': 'I found two recipes for you. The first one is \"Chicken and Bell Pepper Stir Fry\". The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \"Chicken and Rice Casserole\". The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?'}, {'from': 'human', 'value': \"They both sound delicious, but I think I'll try the stir fry. Can you order the ingredients for me?\"}, {'from': 'gpt', 'value': \"I'm sorry, but as an AI, I don't have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.\"}], 'tools': '[{\"name\": \"search_recipes\", \"description\": \"Search for recipes based on ingredients\", \"parameters\": {\"type\": \"object\", \"properties\": {\"ingredients\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The ingredients to search for\"}}, \"required\": [\"ingredients\"]}}]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wD6SnjZeeDB9",
        "outputId": "05562745-5bb2-4d42-e245-baad5acedda1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/LLaMA-Factory'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir configs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpiTRO1CeRE2",
        "outputId": "0f91ae46-8539-4dd9-9529-83bdd30e4655"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘configs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tune config\n",
        "example_config_path = \"/content/LLaMA-Factory/examples/train_lora/llama3_lora_sft_ds0.yaml\" #\"/content/LLaMA-Factory/examples/train_lora/llama3_lora_sft.yaml\"\n",
        "custom_config_path = \"./configs/custom_lora_instruct.json\"\n",
        "\n",
        "with open(example_config_path,\"r\") as f:\n",
        "  args = yaml.safe_load(f)\n",
        "\n",
        "print(json.dumps(args,indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5c9uolDeTlm",
        "outputId": "b195841b-708e-4721-df25-19d3c534304d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"model_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"stage\": \"sft\",\n",
            "  \"do_train\": true,\n",
            "  \"finetuning_type\": \"lora\",\n",
            "  \"lora_target\": \"all\",\n",
            "  \"deepspeed\": \"examples/deepspeed/ds_z0_config.json\",\n",
            "  \"dataset\": \"identity,alpaca_en_demo\",\n",
            "  \"template\": \"llama3\",\n",
            "  \"cutoff_len\": 1024,\n",
            "  \"max_samples\": 1000,\n",
            "  \"overwrite_cache\": true,\n",
            "  \"preprocessing_num_workers\": 16,\n",
            "  \"output_dir\": \"saves/llama3-8b/lora/sft\",\n",
            "  \"logging_steps\": 10,\n",
            "  \"save_steps\": 500,\n",
            "  \"plot_loss\": true,\n",
            "  \"overwrite_output_dir\": true,\n",
            "  \"per_device_train_batch_size\": 1,\n",
            "  \"gradient_accumulation_steps\": 2,\n",
            "  \"learning_rate\": 0.0001,\n",
            "  \"num_train_epochs\": 3.0,\n",
            "  \"lr_scheduler_type\": \"cosine\",\n",
            "  \"warmup_ratio\": 0.1,\n",
            "  \"bf16\": true,\n",
            "  \"ddp_timeout\": 180000000,\n",
            "  \"val_size\": 0.1,\n",
            "  \"per_device_eval_batch_size\": 1,\n",
            "  \"eval_strategy\": \"steps\",\n",
            "  \"eval_steps\": 500\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom example config\n",
        "args[\"model_name_or_path\"] = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "args[\"num_train_epochs\"] = 5\n",
        "args[\"output_dir\"] = \"saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain\"\n",
        "args[\"per_device_train_batch_size\"] = 16 #1,2,4,8,16=1min\n",
        "\n",
        "# Export your custom config\n",
        "json.dump(args, open(custom_config_path, \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "print(json.dumps(args,indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0T__wSSeor9",
        "outputId": "e515ea50-30de-4661-87fb-7bb1803e6ea1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"model_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"stage\": \"sft\",\n",
            "  \"do_train\": true,\n",
            "  \"finetuning_type\": \"lora\",\n",
            "  \"lora_target\": \"all\",\n",
            "  \"deepspeed\": \"examples/deepspeed/ds_z0_config.json\",\n",
            "  \"dataset\": \"identity,alpaca_en_demo\",\n",
            "  \"template\": \"llama3\",\n",
            "  \"cutoff_len\": 1024,\n",
            "  \"max_samples\": 1000,\n",
            "  \"overwrite_cache\": true,\n",
            "  \"preprocessing_num_workers\": 16,\n",
            "  \"output_dir\": \"saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain\",\n",
            "  \"logging_steps\": 10,\n",
            "  \"save_steps\": 500,\n",
            "  \"plot_loss\": true,\n",
            "  \"overwrite_output_dir\": true,\n",
            "  \"per_device_train_batch_size\": 16,\n",
            "  \"gradient_accumulation_steps\": 2,\n",
            "  \"learning_rate\": 0.0001,\n",
            "  \"num_train_epochs\": 5,\n",
            "  \"lr_scheduler_type\": \"cosine\",\n",
            "  \"warmup_ratio\": 0.1,\n",
            "  \"bf16\": true,\n",
            "  \"ddp_timeout\": 180000000,\n",
            "  \"val_size\": 0.1,\n",
            "  \"per_device_eval_batch_size\": 1,\n",
            "  \"eval_strategy\": \"steps\",\n",
            "  \"eval_steps\": 500\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q deepspeed>=0.9.3"
      ],
      "metadata": {
        "id": "hM5sj0-GiQ02"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/LLaMA-Factory/saves"
      ],
      "metadata": {
        "id": "CmtF6M-Oiifx"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "!FORCE_TORCHRUN=1 llamafactory-cli train /content/LLaMA-Factory/configs/custom_lora_instruct.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDuq37IIfNpT",
        "outputId": "4582cb72-8157-4ce3-dc7e-874dc13f1a4e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-03 10:58:14.626954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-03 10:58:14.649237: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-03 10:58:14.656075: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-03 10:58:15.933009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-08-03 10:58:18,835] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n",
            "08/03/2024 10:58:22 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:20238\n",
            "2024-08-03 10:58:27.008210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-03 10:58:27.028757: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-03 10:58:27.035082: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-03 10:58:28.283616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-08-03 10:58:31,129] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n",
            "[2024-08-03 10:58:34,974] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-08-03 10:58:34,975] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "08/03/2024 10:58:34 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "08/03/2024 10:58:34 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 10:58:35,102 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 10:58:35,103 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 10:58:35,103 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 10:58:35,103 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-08-03 10:58:35,473 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "08/03/2024 10:58:35 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "08/03/2024 10:58:35 - INFO - llamafactory.data.loader - Loading dataset identity.json...\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 91/91 [00:00<00:00, 390.82 examples/s]\n",
            "08/03/2024 10:58:36 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 4583.49 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1091/1091 [00:03<00:00, 309.70 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 5991, 609, 39254, 459, 15592, 18328, 8040, 555, 5991, 3170, 3500, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 5991, 609, 39254, 459, 15592, 18328, 8040, 555, 5991, 3170, 3500, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-08-03 10:58:40,736 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-03 10:58:40,737 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "08/03/2024 10:58:40 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[WARNING|quantization_config.py:397] 2024-08-03 10:58:40,767 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:3556] 2024-08-03 10:58:40,769 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-08-03 10:58:40,809 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-08-03 10:58:40,813 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-08-03 10:58:43,663 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-08-03 10:58:43,664 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-08-03 10:58:43,710 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-08-03 10:58:43,710 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "08/03/2024 10:58:43 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "08/03/2024 10:58:43 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "08/03/2024 10:58:43 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "08/03/2024 10:58:43 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "08/03/2024 10:58:43 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,k_proj,v_proj,up_proj,o_proj,q_proj,down_proj\n",
            "08/03/2024 10:58:44 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:642] 2024-08-03 10:58:44,488 >> Using auto half precision backend\n",
            "[2024-08-03 10:58:44,919] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown\n",
            "[2024-08-03 10:58:45,331] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2024-08-03 10:58:45,336] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
            "[2024-08-03 10:58:45,336] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-08-03 10:58:45,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
            "[2024-08-03 10:58:45,391] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer\n",
            "[2024-08-03 10:58:45,728] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer\n",
            "[2024-08-03 10:58:45,729] [INFO] [utils.py:782:see_memory_usage] MA 5.35 GB         Max_MA 5.39 GB         CA 5.39 GB         Max_CA 5 GB \n",
            "[2024-08-03 10:58:45,729] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 4.89 GB, percent = 5.9%\n",
            "[2024-08-03 10:58:46,070] [INFO] [utils.py:781:see_memory_usage] before initializing group 0\n",
            "[2024-08-03 10:58:46,071] [INFO] [utils.py:782:see_memory_usage] MA 5.35 GB         Max_MA 5.35 GB         CA 5.39 GB         Max_CA 5 GB \n",
            "[2024-08-03 10:58:46,071] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 4.89 GB, percent = 5.9%\n",
            "[2024-08-03 10:58:46,424] [INFO] [utils.py:781:see_memory_usage] after initializing group 0\n",
            "[2024-08-03 10:58:46,425] [INFO] [utils.py:782:see_memory_usage] MA 5.51 GB         Max_MA 5.51 GB         CA 5.62 GB         Max_CA 6 GB \n",
            "[2024-08-03 10:58:46,425] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 4.88 GB, percent = 5.9%\n",
            "[2024-08-03 10:58:46,762] [INFO] [utils.py:781:see_memory_usage] before initialize_optimizer\n",
            "[2024-08-03 10:58:46,763] [INFO] [utils.py:782:see_memory_usage] MA 5.51 GB         Max_MA 5.51 GB         CA 5.62 GB         Max_CA 6 GB \n",
            "[2024-08-03 10:58:46,763] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 4.89 GB, percent = 5.9%\n",
            "[2024-08-03 10:58:47,104] [INFO] [utils.py:781:see_memory_usage] end initialize_optimizer\n",
            "[2024-08-03 10:58:47,105] [INFO] [utils.py:782:see_memory_usage] MA 5.51 GB         Max_MA 5.51 GB         CA 5.62 GB         Max_CA 6 GB \n",
            "[2024-08-03 10:58:47,105] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 4.89 GB, percent = 5.9%\n",
            "[2024-08-03 10:58:47,448] [INFO] [utils.py:781:see_memory_usage] end bf16_optimizer\n",
            "[2024-08-03 10:58:47,449] [INFO] [utils.py:782:see_memory_usage] MA 5.51 GB         Max_MA 5.51 GB         CA 5.62 GB         Max_CA 6 GB \n",
            "[2024-08-03 10:58:47,450] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 4.9 GB, percent = 5.9%\n",
            "[2024-08-03 10:58:47,450] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer\n",
            "[2024-08-03 10:58:47,450] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
            "[2024-08-03 10:58:47,450] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2024-08-03 10:58:47,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
            "[2024-08-03 10:58:47,455] [INFO] [config.py:997:print] DeepSpeedEngine configuration:\n",
            "[2024-08-03 10:58:47,456] [INFO] [config.py:1001:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-08-03 10:58:47,456] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-08-03 10:58:47,456] [INFO] [config.py:1001:print]   amp_enabled .................. False\n",
            "[2024-08-03 10:58:47,456] [INFO] [config.py:1001:print]   amp_params ................... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x79c213830fd0>\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   communication_data_type ...... None\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   disable_allgather ............ False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   dump_state ................... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False\n",
            "[2024-08-03 10:58:47,457] [INFO] [config.py:1001:print]   elasticity_enabled ........... False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   fp16_enabled ................. False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   global_rank .................. 0\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 2\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   graph_harvesting ............. False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   loss_scale ................... 1.0\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   memory_breakdown ............. False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   mics_shard_size .............. -1\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   optimizer_name ............... None\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   optimizer_params ............. None\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   pld_enabled .................. False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   pld_params ................... False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   prescale_gradients ........... False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   scheduler_name ............... None\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   scheduler_params ............. None\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   sparse_attention ............. None\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False\n",
            "[2024-08-03 10:58:47,458] [INFO] [config.py:1001:print]   steps_per_print .............. inf\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   train_batch_size ............. 32\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  16\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   use_node_local_storage ....... False\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   weight_quantization_config ... None\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   world_size ................... 1\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   zero_enabled ................. False\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 0\n",
            "[2024-08-03 10:58:47,459] [INFO] [config.py:987:print_user_config]   json = {\n",
            "    \"train_batch_size\": 32, \n",
            "    \"train_micro_batch_size_per_gpu\": 16, \n",
            "    \"gradient_accumulation_steps\": 2, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"zero_allow_untested_optimizer\": true, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 0, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 5.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 5.000000e+08, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"round_robin_gradients\": true\n",
            "    }, \n",
            "    \"steps_per_print\": inf\n",
            "}\n",
            "[INFO|trainer.py:2128] 2024-08-03 10:58:47,459 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-08-03 10:58:47,459 >>   Num examples = 981\n",
            "[INFO|trainer.py:2130] 2024-08-03 10:58:47,459 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:2131] 2024-08-03 10:58:47,459 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:2134] 2024-08-03 10:58:47,459 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2135] 2024-08-03 10:58:47,459 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer.py:2136] 2024-08-03 10:58:47,459 >>   Total optimization steps = 155\n",
            "[INFO|trainer.py:2137] 2024-08-03 10:58:47,466 >>   Number of trainable parameters = 20,971,520\n",
            "{'loss': 1.2752, 'grad_norm': 0.7386122941970825, 'learning_rate': 6.25e-05, 'epoch': 0.32}\n",
            "{'loss': 1.1158, 'grad_norm': 0.49706512689590454, 'learning_rate': 9.979581007037776e-05, 'epoch': 0.65}\n",
            "{'loss': 1.114, 'grad_norm': 0.3315909206867218, 'learning_rate': 9.751778332739033e-05, 'epoch': 0.97}\n",
            "{'loss': 1.0663, 'grad_norm': 0.46863222122192383, 'learning_rate': 9.282275574435281e-05, 'epoch': 1.29}\n",
            "{'loss': 0.9681, 'grad_norm': 0.2766980528831482, 'learning_rate': 8.594954076788736e-05, 'epoch': 1.61}\n",
            "{'loss': 1.0147, 'grad_norm': 0.3775129020214081, 'learning_rate': 7.724774574936188e-05, 'epoch': 1.94}\n",
            "{'loss': 0.9462, 'grad_norm': 0.42581552267074585, 'learning_rate': 6.715998910228296e-05, 'epoch': 2.26}\n",
            "{'loss': 0.9385, 'grad_norm': 0.4225819408893585, 'learning_rate': 5.619938643480561e-05, 'epoch': 2.58}\n",
            "{'loss': 0.9479, 'grad_norm': 0.45262137055397034, 'learning_rate': 4.4923450829394605e-05, 'epoch': 2.9}\n",
            "{'loss': 0.9092, 'grad_norm': 0.5229337811470032, 'learning_rate': 3.390573483674142e-05, 'epoch': 3.23}\n",
            "{'loss': 0.8824, 'grad_norm': 0.7562232613563538, 'learning_rate': 2.3706656619162278e-05, 'epoch': 3.55}\n",
            "{'loss': 0.8362, 'grad_norm': 0.6216592788696289, 'learning_rate': 1.484499417709087e-05, 'epoch': 3.87}\n",
            "{'loss': 0.8331, 'grad_norm': 0.6258602738380432, 'learning_rate': 7.77149761010898e-06, 'epoch': 4.19}\n",
            "{'loss': 0.8218, 'grad_norm': 0.6016384363174438, 'learning_rate': 2.8459616297395466e-06, 'epoch': 4.52}\n",
            "{'loss': 0.7859, 'grad_norm': 0.6237926483154297, 'learning_rate': 3.1892453488058803e-07, 'epoch': 4.84}\n",
            "100% 155/155 [11:53<00:00,  4.04s/it][INFO|trainer.py:3478] 2024-08-03 11:10:45,624 >> Saving model checkpoint to saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155\n",
            "[INFO|configuration_utils.py:733] 2024-08-03 11:10:45,775 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-03 11:10:45,776 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-08-03 11:10:45,863 >> tokenizer config file saved in saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-08-03 11:10:45,863 >> Special tokens file saved in saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/special_tokens_map.json\n",
            "[2024-08-03 11:10:46,197] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step155 is about to be saved!\n",
            "[2024-08-03 11:10:46,258] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/global_step155/mp_rank_00_model_states.pt\n",
            "[2024-08-03 11:10:46,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/global_step155/mp_rank_00_model_states.pt...\n",
            "[2024-08-03 11:10:46,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/global_step155/mp_rank_00_model_states.pt.\n",
            "[2024-08-03 11:10:46,561] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/global_step155/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
            "[2024-08-03 11:10:46,890] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/global_step155/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
            "[2024-08-03 11:10:46,890] [INFO] [engine.py:3478:_save_zero_checkpoint] bf16_zero checkpoint saved saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/checkpoint-155/global_step155/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2024-08-03 11:10:46,890] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step155 is ready now!\n",
            "[INFO|trainer.py:2383] 2024-08-03 11:10:46,893 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 719.4277, 'train_samples_per_second': 6.818, 'train_steps_per_second': 0.215, 'train_loss': 0.9578979246077999, 'epoch': 5.0}\n",
            "100% 155/155 [11:59<00:00,  4.64s/it]\n",
            "[INFO|trainer.py:3478] 2024-08-03 11:10:49,223 >> Saving model checkpoint to saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain\n",
            "[INFO|configuration_utils.py:733] 2024-08-03 11:10:49,337 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-03 11:10:49,338 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-08-03 11:10:49,417 >> tokenizer config file saved in saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-08-03 11:10:49,417 >> Special tokens file saved in saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  total_flos               = 88623722GF\n",
            "  train_loss               =     0.9579\n",
            "  train_runtime            = 0:11:59.42\n",
            "  train_samples_per_second =      6.818\n",
            "  train_steps_per_second   =      0.215\n",
            "Figure saved at: saves/llama-3-8b-Instruct-bnb-4bit/lora/pretrain/training_loss.png\n",
            "08/03/2024 11:10:49 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
            "08/03/2024 11:10:49 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-08-03 11:10:49,812 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-08-03 11:10:49,812 >>   Num examples = 110\n",
            "[INFO|trainer.py:3793] 2024-08-03 11:10:49,812 >>   Batch size = 1\n",
            "100% 110/110 [00:16<00:00,  6.76it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_loss               =     1.0122\n",
            "  eval_runtime            = 0:00:16.43\n",
            "  eval_samples_per_second =      6.692\n",
            "  eval_steps_per_second   =      6.692\n",
            "[INFO|modelcard.py:449] 2024-08-03 11:11:06,250 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/LLaMA-Factory/src')\n",
        "\n",
        "import llamafactory\n",
        "\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc"
      ],
      "metadata": {
        "id": "5pmm_mcpgKmo"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"/saves/llama-3-8b-Instruct-bnb-4bit\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "\n",
        "print(json.dumps(args,indent=2))\n",
        "\n",
        "\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MMrX4scE2PHS",
        "outputId": "8a2cacc8-e406-4021-9ad8-e36a97b4d268"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 11:24:15,219 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 11:24:15,220 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 11:24:15,222 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-03 11:24:15,222 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"model_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"adapter_name_or_path\": \"/saves/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"template\": \"llama3\",\n",
            "  \"finetuning_type\": \"lora\",\n",
            "  \"quantization_bit\": 4\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING|logging.py:313] 2024-08-03 11:24:15,557 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/03/2024 11:24:15 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.data.template:Replace eos token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-08-03 11:24:15,609 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-03 11:24:15,610 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/03/2024 11:24:15 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llamafactory.model.model_utils.quantization:`quantization_bit` will not affect on the PTQ-quantized models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/03/2024 11:24:15 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.model_utils.quantization:Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/03/2024 11:24:15 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.patcher:Using KV cache for faster generation.\n",
            "[WARNING|quantization_config.py:397] 2024-08-03 11:24:15,620 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-0e0048fcaf3c>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mchat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/LLaMA-Factory/src/llamafactory/chat/chat_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetuning_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerating_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_infer_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"BaseEngine\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingfaceEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetuning_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerating_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vllm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"BaseEngine\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVllmEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetuning_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerating_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/LLaMA-Factory/src/llamafactory/chat/hf_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_args, data_args, finetuning_args, generating_args)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_side\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"left\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_generate\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_template_and_fix_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtool_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         self.model = load_model(\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetuning_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_trainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_valuehead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         )  # must after fixing tokenizer to resize vocab\n",
            "\u001b[0;32m/content/LLaMA-Factory/src/llamafactory/model/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(tokenizer, model_args, finetuning_args, is_trainable, add_valuehead)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixture_of_depths\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"convert\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3279\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3280\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2r7vUd8i-1Y1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}